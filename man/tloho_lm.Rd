% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tloho_lm.R
\name{tloho_lm}
\alias{tloho_lm}
\title{tloho_lm: Tree-based Low rank Horseshoe regularization prior in linear model}
\usage{
tloho_lm(
  Y,
  X,
  intercept = F,
  scale = T,
  graph0,
  init_val = NULL,
  c = 0.5,
  tau0 = 1,
  intercept_var = 10000,
  nsave = 1000,
  nburn = 40000,
  nthin = 10,
  verbose = 1000,
  loss = "binder",
  hsplus = F,
  seed = NULL
)
}
\arguments{
\item{Y}{n by 1 scalar, real-valued response variables.}

\item{X}{n by p matrix, real-valued predictors.}

\item{intercept}{logical (default F), add 1's at the first column of X ? See intercept_var for variance.}

\item{scale}{logical (default T), if true X will be standardized so that all columns have L2 norm 1. currently cannot combined with scale = T}

\item{graph0}{igraph object, reflecting the structure of beta.}

\item{init_val}{list (default NULL), list with name 'beta' and/or 'trees', Initial value of beta and/or spanning forest.}

\item{c}{real between 0 and 1, (default 0.5), Model size penalization hyperparameter}

\item{tau0}{positive real (default 1), shrinkage strength hyperparameter}

\item{intercept_var}{positive real (default 10000). Set variance of the mean zero normal prior on the intercept as sigma2*intercept_var, where sigma2 is error variance parameter. Ignored if intercept = F}

\item{nsave}{integer (default 1000), number of posteror sample saved. Total number of MCMC iteration = nburn + nsave*nthin.}

\item{nburn}{integer (default 40000), number of burn-in iteration. Total number of MCMC iteration = nburn + nsave*nthin.}

\item{nthin}{integer (default 10) thin-in rate. Total number of MCMC iteration = nburn + nsave*nthin.}

\item{verbose}{integer (default 1000), print progress at every "verbose" iteration. Set negative (e.g. verbose = -1) to turn off.}

\item{loss}{"binder" or "VI" (default "binder"), whether to calculate cluster estimate based on Binder loss (default) or VI loss}

\item{hsplus}{logical (default F), use horseshoe+ prior instead of horseshoe? (experimental)}

\item{seed}{seed (default NULL)}
}
\value{
A list containing:\tabular{ll}{
   \code{beta_out} \tab nsamples by p matrix, posterior samples of beta\cr
   \tab \cr
   \code{lambda2_out} \tab list of length nsamples, posterior samples of lambda^2 \cr
   \tab \cr
   \code{tau2_out} \tab length nsamples vector, posterior samples of tau^2 \cr
   \tab \cr
   \code{sigmasq_y_out} \tab length nsamples vector, posterior samples of sigma^2 \cr
   \tab \cr
   \code{cluster_out} \tab nsamples by p matrix, posterior samples of cluster label \cr
   \tab \cr
   \code{cluster_map} \tab length p vector, maximum a posteriori cluster estimate \cr
   \tab \cr
   \code{cluster_est} \tab length p vector, cluster estimate based on Bayes estimator that minimizes expected loss \cr
   \tab \cr
   \code{log_post_out} \tab length nsamples vector, log posterior likelihood(up to a constant) \cr
   \tab \cr
   \code{acc} \tab list with split/merge/change/hyper move proposal counts and acceptance counts \cr
   \tab \cr
   \code{map_beta_est} \tab length p vector, maximum a posteriori beta estimate\cr
   \tab \cr
   \code{mean_beta_est} \tab length p vector, posterior mean beta estimate \cr
   \tab \cr
   \code{median_beta_est} \tab length p vector, posterior median beta estimate \cr
   \tab \cr
   \code{map_MSF_est} \tab igraph object, a sample of minimum spanning forest when posterior likelihood is maximized\cr
   \tab \cr
}
}
\description{
tloho_lm: Tree-based Low rank Horseshoe regularization prior in linear model
}
\details{
The main R function of the paper 

T-LoHo: A Bayesian Regularization Model for Structured Sparsity and Smoothness on Graphs
by C. J. Lee, Z. T. Luo, and H. Sang, NeurIPS 2021
}
